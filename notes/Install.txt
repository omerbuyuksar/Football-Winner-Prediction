https://chawlasumit.wordpress.com/2015/03/09/install-a-multi-node-hadoop-cluster-on-ubuntu-14-04/


sudo nano /etc/hosts
#bunları ekle ipler
52.34.48.241 hadoop-master
52.32.73.62 hadoop-slave-1


$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install oracle-java7-installer
# Updata Java runtime
$ sudo update-java-alternatives -s java-7-oracle

# Create hadoopgroup
$ sudo addgroup hadoopgroup
# Create hadoopuser user
$ sudo adduser —ingroup hadoopgroup hadoopuser

# Login as hadoopuser
$ su - hadoopuser
#Generate a ssh key for the user
$ ssh-keygen -t rsa -P ""
#Authorize the key to enable password less ssh 
$ cat /home/hadoopuser/.ssh/id_rsa.pub >> /home/hadoopuser/.ssh/authorized_keys
$ chmod 600 ~/.ssh/authorized_keys
#Copy this key to slave-1 to enable password less ssh 
$ ssh-copy-id -i ~/.ssh/id_rsa.pub slave-1
#Make sure you can do a password less ssh using following command.
$ ssh slave-1
  (burda denied public key hatası alıoyrum) single node olsun

  cd 
  wget http://ftp.itu.edu.tr/Mirror/Apache/hadoop/common/hadoop-2.6.3/hadoop-2.6.3.tar.gz
  tar xvf hadoop-2.6.3.tar.gz 
  mv hadoop-2.6.3 hadoop

  nano ~/.bashrc
# bunları ekle
# Set HADOOP_HOME
export HADOOP_HOME=/home/hadoopuser/hadoop
# Set JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-7-oracle
# Add Hadoop bin and sbin directory to PATH
export PATH=$PATH:$HADOOP_HOME/bin;$HADOOP_HOME/sbin

nano hadoop/etc/hadoop/hadoop-env.sh  (bunda düzenle java home)
export JAVA_HOME=/usr/lib/jvm/java-7-oracle

gerekli yerlere ekle 
nano /home/hadoopuser/hadoop/etc/hadoop/core-site.xml
<property>
  <name>hadoop.tmp.dir</name>
  <value>/home/hadoopuser/tmp</value>
  <description>Temporary Directory.</description>
</property>

<property>
  <name>fs.defaultFS</name>
  <value>hdfs://master:54310</value>
  <description>Use HDFS as file storage engine</description>
</property>


cp /home/hadoopuser/hadoop/etc/hadoop/mapred-site.xml.template /home/hadoopuser/hadoop/etc/hadoop/mapred-site.xml
nano /home/hadoopuser/hadoop/etc/hadoop/mapred-site.xml
<property>
 <name>mapreduce.jobtracker.address</name>
 <value>master:54311</value>
 <description>The host and port that the MapReduce job tracker runs
  at. If “local”, then jobs are run in-process as a single map
  and reduce task.
</description>
</property>
<property>
 <name>mapreduce.framework.name</name>
 <value>yarn</value>
 <description>The framework for running mapreduce jobs</description>
</property>


mkdir hdfs
nano  /home/hadoopuser/hadoop/etc/hadoop/hdfs-site.xml

<property>
 <name>dfs.replication</name>
 <value>2</value>
 <description>Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
 </description>
</property>
<property>
 <name>dfs.namenode.name.dir</name>
 <value>/home/hadoopuser/hdfs/namenode</value>
 <description>Determines where on the local filesystem the DFS name node should store the name table(fsimage). If this is a comma-delimited list of d$
 </description>
</property>
<property>
 <name>dfs.datanode.data.dir</name>
 <value>/home/hadoopuser/hdfs/datanode</value>
 <description>Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, th$
 </description>
</property>

nano /home/hadoopuser/hadoop/etc/hadoop/yarn-site.xml

<property>
 <name>yarn.nodemanager.aux-services</name>
 <value>mapreduce_shuffle</value>
</property>
<property>
 <name>yarn.resourcemanager.scheduler.address</name>
 <value>master:8030</value>
</property> 
<property>
 <name>yarn.resourcemanager.address</name>
 <value>master:8032</value>
</property>
<property>
  <name>yarn.resourcemanager.webapp.address</name>
  <value>master:8088</value>
</property>
<property>
  <name>yarn.resourcemanager.resource-tracker.address</name>
  <value>master:8031</value>
</property>
<property>
  <name>yarn.resourcemanager.admin.address</name>
  <value>master:8033</value>
</property>


nano  /home/hadoopuser/hadoop/etc/hadoop/slaves
master

hdfs namenode -format 
#üstteki olmazsa hadoop bin'den yap
./hadoop/sbin/start-all.sh
jps















